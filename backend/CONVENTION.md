
### 1. ì „ì²´ êµ¬ì¡° & Router ì»¨ë²¤ì…˜
**ğŸ“ êµ¬ì¡° ì˜ˆì‹œ**
```bash
app/
  core/         # ê³µí†µ ì„¤ì •, ë¯¸ë“¤ì›¨ì–´, config
  api/
    v1/
      router.py # version ë‹¨ìœ„ Router ì§‘ê³„
      endpoints/
        inference.py
        health.py
  services/     # LLM, Embedding í´ë¼ì´ì–¸íŠ¸/ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
  schemas/      # Pydantic ëª¨ë¸ë“¤
  config.py
  main.py
```
**ğŸ”— Router íŒ¨í„´**

APIRouter + prefix + tags ì¡°í•©ì´ ê±°ì˜ í‘œì¤€
```python
# app/api/v1/endpoints/inference.py
from fastapi import APIRouter, Depends
from app.schemas.inference import ChatRequest, ChatResponse
from app.services.chat_service import ChatService, get_chat_service

router = APIRouter()

@router.post(
    "/chat/completions",
    response_model=ChatResponse,
    summary="LLM Chat Completion",
)
async def create_chat_completion(
    payload: ChatRequest,
    svc: ChatService = Depends(get_chat_service),
):
    return await svc.generate(payload)
```
```python
# app/api/v1/router.py
from fastapi import APIRouter
from app.api.v1.endpoints import inference, health

api_v1_router = APIRouter(prefix="/api/v1")

api_v1_router.include_router(inference.router, prefix="/inference", tags=["inference"])
api_v1_router.include_router(health.router, prefix="/health", tags=["health"])
```
### 2. íƒ€ì… íŒíŠ¸ & ì–´ë…¸í…Œì´ì…˜ ìŠ¤íƒ€ì¼
**âœ… ê¸°ë³¸ ìŠ¤íƒ€ì¼**

- í•¨ìˆ˜ íŒŒë¼ë¯¸í„°/ë¦¬í„´ ëª¨ë‘ íƒ€ì… ëª…ì‹œ  
- I/OëŠ” Pydantic BaseModelë¡œ í†µì¼
- DIëŠ” Depends() ì‚¬ìš©

```python
from typing import List, Literal, Optional
from pydantic import BaseModel, Field

class Message(BaseModel):
    role: Literal["user", "assistant", "system"]
    content: str

class ChatRequest(BaseModel):
    model: str = Field(..., description="Model name")
    messages: List[Message]
    temperature: float = 0.7
    max_tokens: int = 512

class ChatResponse(BaseModel):
    id: str
    model: str
    output: str
    finish_reason: str
```

ë¦¬í„´ íƒ€ì…ë„ ëª…ì‹œ:
```python
from fastapi import status

@router.post(
    "/chat/completions",
    response_model=ChatResponse,
    status_code=status.HTTP_200_OK,
)
async def create_chat_completion(...) -> ChatResponse:
    ...
```

### 3. ì…ë ¥/ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì»¨ë²¤ì…˜ (AI ê²Œì´íŠ¸ì›¨ì´ìš©)
**ğŸ“¥ ì¸í’‹ ìŠ¤íƒ€ì¼ (ì¶”ì²œ)**

OpenAI ìŠ¤íƒ€ì¼ ë”°ë¼ê°€ëŠ” ê²Œ ì œì¼ ë¬´ë‚œí•¨

ê³µí†µ í•„ë“œ:
- model
- input or messages
- parameters(temperature, top_p, max_tokens ë“±)
- metadata(trace id, user id ë“±)
```python
class EmbeddingRequest(BaseModel):
    model: str
    input: list[str]  # ë°°ì¹˜ ì²˜ë¦¬ ê¸°ë³¸
    user: Optional[str] = None
```

**ğŸ“¤ ì•„ì›ƒí’‹ ìŠ¤íƒ€ì¼**

ê³µí†µ í•„ë“œ:
- id (ìš”ì²­ ID)
- model
- data (ë°°ì—´)
- usage (token/stat info)
- created (timestamp)
```python
class EmbeddingItem(BaseModel):
    index: int
    embedding: list[float]

class Usage(BaseModel):
    prompt_tokens: int
    total_tokens: int

class EmbeddingResponse(BaseModel):
    id: str
    model: str
    data: list[EmbeddingItem]
    usage: Usage
    created: int
```

ğŸ‘‰ ì´ë ‡ê²Œ í•´ë‘ë©´ LLM/Embedding/Rerank ì „ë¶€ ë™ì¼ íŒ¨í„´ìœ¼ë¡œ ê´€ë¦¬ ê°€ëŠ¥.

### 4. DI / Service ë¶„ë¦¬ ì»¨ë²¤ì…˜
**âœ‚ ì„œë¹„ìŠ¤ ë ˆì´ì–´ ë¶„ë¦¬**

ì—”ë“œí¬ì¸íŠ¸ëŠ” â€œì…ì¶œë ¥ + HTTPë§Œâ€ ì²˜ë¦¬ ì‹¤ì œ LLM/Vector/ì„œë“œíŒŒí‹° í˜¸ì¶œì€ services/ ì—ì„œ

```python
# app/services/chat_service.py
from app.schemas.inference import ChatRequest, ChatResponse

class ChatService:
    def __init__(self, llm_client: "LLMClient"):
        self.llm_client = llm_client

    async def generate(self, payload: ChatRequest) -> ChatResponse:
        llm_result = await self.llm_client.chat(payload)
        return ChatResponse(
            id=llm_result.id,
            model=payload.model,
            output=llm_result.output,
            finish_reason=llm_result.finish_reason,
        )

def get_chat_service() -> ChatService:
    # ë‚˜ì¤‘ì— DI ì»¨í…Œì´ë„ˆë¡œ êµì²´ ê°€ëŠ¥
    from app.clients.llm_client import LLMClient
    client = LLMClient()
    return ChatService(client)
```

ì—”ë“œí¬ì¸íŠ¸ì—ì„œëŠ” í•­ìƒ Depends(get_chat_service) ì‚¬ìš©

### 5. ì—ëŸ¬/ì˜ˆì™¸ ì‘ë‹µ ì»¨ë²¤ì…˜
**ğŸ§± ê³µí†µ ì—ëŸ¬ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ**
```python
class ErrorResponse(BaseModel):
    code: str
    message: str
    details: Optional[dict] = None
```
HTTPException + ì „ì—­ í•¸ë“¤ëŸ¬
```python
from fastapi import HTTPException
from fastapi.responses import JSONResponse
from fastapi.requests import Request
from fastapi import FastAPI

app = FastAPI()

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            code="INTERNAL_SERVER_ERROR",
            message="Unexpected error",
        ).model_dump(),
    )
```

ë„ë©”ì¸ ì—ëŸ¬(LLM íƒ€ì„ì•„ì›ƒ ë“±)ëŠ” ì»¤ìŠ¤í…€ ì˜ˆì™¸ ì •ì˜í•´ì„œ 4xx/5xx ë¶„ë¦¬

### 6. ì½”ë“œ ìŠ¤íƒ€ì¼ (í•¨ìˆ˜/íŒŒì¼/ë„¤ì´ë°)

**íŒŒì¼ëª…: snake_case**
- chat_service.py, embedding_client.py, inference.py

**í´ë˜ìŠ¤ëª…: PascalCase**
- ChatService, EmbeddingClient

**ì—”ë“œí¬ì¸íŠ¸ í•¨ìˆ˜:**
- ì˜ë¯¸ + HTTP ë©”ì„œë“œ ì¡°í•©
- create_chat_completion, get_model_list, health_check ë“±

**ì—”ë“œí¬ì¸íŠ¸ í•œ í•¨ìˆ˜ì—:**
- íŒŒë¼ë¯¸í„° ìˆ˜ ìµœì†Œí™”
- ë°”ë””ëŠ” ë¬´ì¡°ê±´ BaseModel í•˜ë‚˜ë¡œ ë¬¶ê¸°
```python
@router.post("/embeddings", response_model=EmbeddingResponse)
async def create_embeddings(
    payload: EmbeddingRequest,
    svc: EmbeddingService = Depends(get_embedding_service),
) -> EmbeddingResponse:
    return await svc.embed(payload)
```
### 7. ë¯¸ë“¤ì›¨ì–´ & ê³µí†µ ì²˜ë¦¬ (AI ê²Œì´íŠ¸ì›¨ì´ì—ì„œ ê±°ì˜ í•„ìˆ˜)
**1) ìš”ì²­/ì‘ë‹µ ë¡œê¹… + trace_id**
```python
from starlette.middleware.base import BaseHTTPMiddleware
import time, uuid
import logging

logger = logging.getLogger(__name__)

class LoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        trace_id = request.headers.get("X-Trace-Id", str(uuid.uuid4()))
        request.state.trace_id = trace_id

        start = time.time()
        response = await call_next(request)
        duration = time.time() - start

        logger.info(
            f"[{trace_id}] {request.method} {request.url.path} "
            f"status={response.status_code} duration={duration:.3f}s"
        )
        response.headers["X-Trace-Id"] = trace_id
        return response

app.add_middleware(LoggingMiddleware)
```
**2) CORS**
```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],        # í™˜ê²½ì— ë”°ë¼ ì¡°ì •
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```
### 8. AI ì„œë¹™ ê²Œì´íŠ¸ì›¨ì´ íŠ¹í™” íŒ
**ğŸ”¹ 1) ë°°ì¹˜ ì…ë ¥ ê¸°ë³¸**
- Embedding / rerank / scoring APIëŠ” ë‹¨ì¼ inputë³´ë‹¤ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ë³¸
```python
class RerankRequest(BaseModel):
    query: str
    documents: list[str]
```
**ğŸ”¹ 2) timeout / retry ì •ì±…ì€ ì„œë¹„ìŠ¤ ë ˆì´ì–´ì—ì„œ**
- FastAPI ë ˆë²¨ì´ ì•„ë‹ˆë¼ LLMClient ì—ì„œ httpx.AsyncClient(timeout=...) ì‚¬ìš©

**ğŸ”¹ 3) ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (LLM)**
- ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ StreamingResponse íŒ¨í„´ ì‚¬ìš©
- ì²˜ìŒ ë²„ì „ì€ ë¹„ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹¨ìˆœí•˜ê²Œ ì‹œì‘ â†’ ì´í›„ í™•ì¥

**ğŸ”¹ 4) ë²„ì „ë³„ ë¶„ë¦¬**
- /api/v1/... /api/v2/... êµ¬ì¡°ë¡œ schema ë³€ê²½ì„ ì œì–´
- ê°™ì€ pathë¼ë„ ëª¨ë¸ë§Œ ë°”ë€” ìˆ˜ ìˆê²Œ ì„¤ê³„

### ìš”ì•½ ì •ë¦¬
1. AI ì„œë¹™ ê²Œì´íŠ¸ì›¨ì´ FastAPIì—ì„œ â€œë§ì´ ì“°ëŠ”/ìœ ìš©í•œâ€ ì»¨ë²¤ì…˜ì€:
2. APIRouter + prefix + tags + ë²„ì „(/api/v1)
3. ëª¨ë“  I/Oë¥¼ Pydantic ëª¨ë¸ë¡œ (OpenAI ìŠ¤íƒ€ì¼ ìŠ¤í‚¤ë§ˆ ì¶”ì²œ)
4. ì„œë¹„ìŠ¤ ë ˆì´ì–´ ë¶„ë¦¬ + Depends() ë¡œ DI
5. response_model, status_codeë¥¼ ë°ì½”ë ˆì´í„°ì—ì„œ ëª…ì‹œ
6. ê³µí†µ ErrorResponse + ì „ì—­ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
7. ë¡œê¹…/trace-id/CORS ê°™ì€ ê³µí†µ ì²˜ë¦¬ ë¯¸ë“¤ì›¨ì–´
8. Embedding/RerankëŠ” ë°°ì¹˜ ì…ë ¥ì´ ê¸°ë³¸, LLMì€ OpenAI ë¹„ìŠ·í•œ messages íŒ¨í„´